{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"functions.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNSzkOXEM2/kgfswBMH+DPi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"QTIFQX7sLvPK","executionInfo":{"status":"ok","timestamp":1620718384284,"user_tz":420,"elapsed":2601,"user":{"displayName":"Babhru Joshi","photoUrl":"","userId":"09323046707416982964"}}},"source":["import plac\n","from tqdm import tqdm\n","import numpy as np\n","from PIL import Image\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import pdb; \n","import math\n","from math import nan"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQB3MybpUIFv","executionInfo":{"status":"ok","timestamp":1620722257631,"user_tz":420,"elapsed":255,"user":{"displayName":"Babhru Joshi","photoUrl":"","userId":"09323046707416982964"}}},"source":["class Generator_d(nn.Module):\n","\n","  def __init__(self, net_param):\n","\n","    super(Generator_d, self).__init__()\n","    self.f = []\n","    for i in range(1,len(net_param)):\n","      net_param[i-1]\n","      layer = nn.Linear(net_param[i-1], net_param[i], bias = False)\n","      nn.init.normal_(list(layer.parameters())[0].data, 0, 1/np.sqrt(net_param[i]))\n","      self.f = np.append(self.f, [layer])\n","    \n","  def decoder(self, z):\n","    for i in range(len(self.f)):\n","      z = F.relu(self.f[i](z))\n","    return z \n","  \n","  def forward(self, z):\n","    return self.decoder(z)\n"],"execution_count":128,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rpe1XHg4iFl5","executionInfo":{"status":"ok","timestamp":1620722078330,"user_tz":420,"elapsed":288,"user":{"displayName":"Babhru Joshi","photoUrl":"","userId":"09323046707416982964"}},"outputId":"03b5b90e-74cc-4cf5-806e-33ba383370c0"},"source":["expanse_param = 5\n","z_dim = 20\n","depth = 5\n","net_param = np.append([z_dim], np.ones(depth)*z_dim*expanse_param)\n","net_param.astype(int)\n","t = np.ones(10)\n","k = [1, 1, 1]\n","len(t)"],"execution_count":113,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"code","metadata":{"id":"_lQa8TIFKXb3"},"source":["class Generator3(nn.Module):\n","\n","  def __init__(self, net_param):\n","\n","    super(Generator3, self).__init__()\n","    self.f1 = nn.Linear(net_param[0], net_param[1], bias = False)\n","    self.f2 = nn.Linear(net_param[1], net_param[2], bias = False)\n","    self.f3 = nn.Linear(net_param[2], net_param[3], bias = False)\n","    \n","  def decoder(self, z):\n","    z = F.relu(self.f1(z))\n","    z = F.relu(self.f2(z))\n","    z = F.relu(self.f3(z))\n","    return z \n","\n","  # def decoder(self, z):\n","  #   z = self.f1(z)\n","  #   z = self.f2(z)\n","  #   z = self.f3(z)\n","  #   return z  \n","  \n","  def forward(self, z):\n","    return self.decoder(z)\n","\n","def weights_init(m):\n","  classname = m.__class__.__name__\n","\n","  # for every Linear layer in a model..\n","  if classname.find('Linear') != -1: \n","    n = m.out_features     # get the number of the inputs\n","    y = 1.0/np.sqrt(n)\n","    m.weight.data.normal_(0, y)\n","\n","class Generator3_sigmoid(nn.Module):\n","\n","  def __init__(self, net_param):\n","\n","    super(Generator3_sigmoid, self).__init__()\n","    self.f1 = nn.Linear(net_param[0], net_param[1], bias = False)\n","    self.f2 = nn.Linear(net_param[1], net_param[2], bias = False)\n","    self.f3 = nn.Linear(net_param[2], net_param[3], bias = False)\n","    \n","  def decoder(self, z):\n","    z = F.sigmoid(self.f1(z))\n","    z = F.sigmoid(self.f2(z))\n","    z = F.sigmoid(self.f3(z))\n","    return z \n","  \n","  def forward(self, z):\n","    return self.decoder(z)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyIkV5xPLEtc","executionInfo":{"status":"ok","timestamp":1620722753308,"user_tz":420,"elapsed":271,"user":{"displayName":"Babhru Joshi","photoUrl":"","userId":"09323046707416982964"}}},"source":["class algo_parameter:\n","  def __init__(self, stepsize, max_itt, tolerance):\n","    self.stepsize = stepsize\n","    self.max_itt = max_itt\n","    self.tolerance = tolerance\n","    \n","class weight_mats():\n","  W = {} # create a data struct for weight matrices\n","  def __init__(self, net_param):\n","    for i in range(len(net_param)-1):\n","      self.W[i] =torch.randn(net_param[i+1],net_param[i])"],"execution_count":134,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILtNF3rwLGAw"},"source":["## function that solves gradient descent algorithm withough flipping with stoping criteria as successive error\n","def gradient_desc(net_param, algo_param, generator, y0, A, out_toggle):\n","  x_dim = net_param[len(net_param)-1]\n","  z_dim = net_param[0]\n","  device = \"cuda\"\n","  z = torch.randn(z_dim, device=device, requires_grad = True)\n","  z_temp = torch.randn(z_dim, device=device, requires_grad = False)\n","  succ_error = 1\n","  itt = 1\n","  while itt < algo_param.max_itt and succ_error > algo_param.tolerance:\n","\n","    z_temp.data = z.clone()\n","    y_pred = torch.matmul(A, generator(z).reshape(x_dim))\n","    loss_fn = (y_pred-y0).pow(2).mean()\n","    \n","    loss_fn.backward()\n","    grad_norm = torch.norm(z.grad)\n","    with torch.no_grad():\n","      z -= algo_param.stepsize*z.grad\n","      z.grad.zero_()\n","\n","    itt += 1\n","    succ_error = torch.norm(z_temp-z)/torch.norm(z)  \n","    \n","    # pdb.set_trace()\n","\n","    if out_toggle != 0:\n","      if itt % ((out_toggle // 10)*10 +1) == 0:\n","        print('====> In gradient descent: Iteration: {} Successive error: {:.4e}'.format(itt, succ_error))\n","\n","  return z, itt\n","\n","## function that solves gradient descent algorithm withough flipping with stopping criteria as relative error\n","def gradient_desc_rel(net_param, algo_param, generator, y0, A, z0, out_toggle):\n","  x_dim = net_param[len(net_param)-1]\n","  z_dim = net_param[0]\n","  device = \"cuda\"\n","  z = torch.randn(z_dim, device=device, requires_grad = True)\n","  rel_error = 1\n","  itt = 1\n","  while itt < algo_param.max_itt and rel_error > algo_param.tolerance:\n","\n","    y_pred = torch.matmul(A, generator(z).reshape(x_dim))\n","    loss_fn = (y_pred-y0).pow(2).mean()\n","    \n","    loss_fn.backward()\n","    grad_norm = torch.norm(z.grad)\n","    with torch.no_grad():\n","      z -= algo_param.stepsize*z.grad\n","      z.grad.zero_()\n","\n","    itt += 1\n","    rel_error = torch.norm(z0-z)/torch.norm(z0)  \n","    \n","    # pdb.set_trace()\n","\n","    if out_toggle != 0:\n","      if itt % ((out_toggle // 10)*10 +1) == 0:\n","        print('====> In gradient descent: Iteration: {} Successive error: {:.4e}'.format(itt, rel_error))\n","\n","  return z, itt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQyCQYggKEAO"},"source":["## function that solves quasi-gradient algorithm with successive error as stopping criteria\n","def quasi_grad(net_param, algo_param, generator, y0, weight_mats_d3, A, out_toggle):\n","  x_dim = net_param[len(net_param)-1]\n","  z_dim = net_param[0]\n","  [m,_] = A.size()\n","  Wm = weight_mats_d3.W[0]\n","  scale = 1 # this will change if the weight matrices are not normalized\n","  depth = len(weight_mats_d3.W)\n","  for i in range(len(weight_mats_d3.W) - 1):\n","    Wm = torch.matmul(weight_mats_d3.W[i+1],Wm)\n","  zk = torch.randn(1, z_dim).cuda()\n","  # zk = .5*zk/torch.norm(zk,2)\n","  z_temp = zk\n","  \n","  itt = 0\n","  succ_error = 1\n","  # pdb.set_trace()\n","  while itt < algo_param.max_itt and succ_error > algo_param.tolerance:\n","    xk = generator(zk).reshape(x_dim)\n","    d = torch.matmul(torch.t(torch.matmul(A,Wm)), torch.matmul(A,xk) -  y0.reshape(m)) * (scale**(-1))\n","    step = algo_param.stepsize *  d\n","    z_temp = zk - torch.t(step)\n","    itt += 1\n","\n","    succ_error = torch.norm(z_temp-zk)/torch.norm(zk)\n","    zk = z_temp\n","    \n","    if out_toggle != 0:\n","      if itt % ((out_toggle // 10)*10 +1) == 0:\n","        print('====> In quasi-gradient: Iteration: {} Successive error: {:.4e}'.format(itt, succ_error))\n","  return zk, itt\n","\n","## function that solves quasi-gradient algorithm with relative error as stopping criteria\n","def quasi_grad_rel(net_param, algo_param, generator, y0, weight_mats_d3, A, z0, out_toggle):\n","  x_dim = net_param[len(net_param)-1]\n","  z_dim = net_param[0]\n","  [m,_] = A.size()\n","\n","  Wm = weight_mats_d3.W[0]\n","  scale = 1 # this will change if the weight matrices are not normalized\n","  depth = len(weight_mats_d3.W)\n","  for i in range(len(weight_mats_d3.W) - 1):\n","    Wm = torch.matmul(weight_mats_d3.W[i+1],Wm)\n","  # zk = torch.zeros(1, z_dim).cuda()\n","  zk = torch.zeros(1, z_dim)\n","  \n","  itt = 0\n","  rel_error = 1\n","  while itt < algo_param.max_itt and rel_error > algo_param.tolerance:\n","    xk = generator(zk).reshape(x_dim)\n","    d = torch.matmul(torch.t(torch.matmul(A,Wm)), torch.matmul(A,xk) -  y0.reshape(m)) * (scale**(-1))\n","    step = algo_param.stepsize *  d\n","    zk = zk - torch.t(step)\n","    itt += 1\n","    rel_error = torch.norm(z0-zk)/torch.norm(z0)\n","\n","    \n","    if out_toggle != 0:\n","      if itt % ((out_toggle // 10)*10 +1) == 0:\n","        print('====> In quasi-gradient: Iteration: {} Relative error: {:.4e}'.format(itt, rel_error))\n","  if out_toggle != 0:   \n","    print('====> In quasi-gradient: Iteration: {} Relative error: {:.4e}'.format(itt, rel_error))  \n","    print('')\n","  # pdb.set_trace()\n","  return zk, itt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_YH2umzAHmr"},"source":[""],"execution_count":null,"outputs":[]}]}