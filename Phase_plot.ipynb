{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Phase_plot.ipynb","provenance":[],"authorship_tag":"ABX9TyOQIhuIH48/zaS5EzrnuAK+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"SUIzhHj72ICY"},"source":["# import packages, load function and connect to google drive\n","import plac\n","from tqdm import tqdm\n","import numpy as np\n","from PIL import Image\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import pdb; \n","import math\n","from math import nan\n","\n","import sys\n","if 'google.colab' in sys.modules and 'torch' not in sys.modules:\n","    from os.path import exists\n","    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n","\n","%matplotlib inline\n","import os\n","\n","# # Load the Drive helper and mount\n","# from google.colab import drive\n","\n","# # This will prompt for authorization.\n","# drive.mount('/content/drive', force_remount=True)\n","\n","# %cd \"/content/drive/My Drive/Colab Notebooks/Linear_approx\"\n","\n","# !pip install import-ipynb\n","# import import_ipynb\n","# import importlib\n","\n","# import functions\n","# importlib.reload(functions)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"joB_xia22LlW"},"source":["class Generator_d(nn.Module):\n","\n","  def __init__(self, net_param):\n","\n","    super(Generator_d, self).__init__()\n","    self.f = []\n","    for i in range(1,len(net_param)):\n","      net_param[i-1]\n","      layer = nn.Linear(net_param[i-1], net_param[i], bias = False)\n","      nn.init.normal_(list(layer.parameters())[0].data, 0, 1/np.sqrt(net_param[i]))\n","      self.f = np.append(self.f, [layer])\n","    \n","  def decoder(self, z):\n","    for i in range(len(self.f)):\n","      z = F.relu(self.f[i](z))\n","    return z \n","  \n","  def forward(self, z):\n","    return self.decoder(z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6twZ1vH02NaA"},"source":["class algo_parameter:\n","  def __init__(self, stepsize, max_itt, tolerance):\n","    self.stepsize = stepsize\n","    self.max_itt = max_itt\n","    self.tolerance = tolerance\n","    \n","class weight_mats():\n","  W = {} # create a data struct for weight matrices\n","  def __init__(self, net_param):\n","    for i in range(len(net_param)-1):\n","      self.W[i] =torch.randn(net_param[i+1],net_param[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8Oo3bEQ2SBt"},"source":["## function that solves quasi-gradient algorithm with successive error as stopping criteria\n","def quasi_grad(net_param, algo_param, generator, y0, Wm, A, out_toggle):\n","  x_dim = net_param[len(net_param)-1]\n","  z_dim = net_param[0]\n","  [m,_] = A.size()\n","  scale = 1 # this will change if the weight matrices are not normalized\n","\n","\n","  zk = torch.randn(1, z_dim).cuda()\n","  z_temp = zk\n","  \n","  itt = 0\n","  succ_error = 1\n","  while itt < algo_param.max_itt and succ_error > algo_param.tolerance:\n","    z_temp = zk - torch.t(algo_param.stepsize *  torch.matmul(torch.t(torch.matmul(A,Wm)), torch.matmul(A, generator(zk).reshape(x_dim)) -  y0.reshape(m)) * (scale**(-1)))\n","    itt += 1\n","    succ_error = torch.norm(z_temp-zk)/torch.norm(zk)\n","    zk.data = z_temp.data\n","    # pdb.set_trace()\n","\n","    \n","    if out_toggle != 0:\n","      if itt % ((out_toggle // 10)*10 +1) == 0:\n","        print('====> In quasi-gradient: Iteration: {} Successive error: {:.4e}'.format(itt, succ_error))\n","  if out_toggle != 0:\n","    print('====> In quasi-gradient: Iteration: {} Successive error: {:.4e}'.format(itt, succ_error))  \n","    print('')\n","\n","  return zk, itt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y07z01pb2Uvk"},"source":["tot_trials = 20\n","depth = 3\n","z_dim_list = range(1,20,1)\n","x_dim_list = range(1,442,20)\n","error_matrix_quasi = torch.zeros(len(z_dim_list), len(x_dim_list), tot_trials)\n","quasi_algo_param = algo_parameter(3, 10000, 1e-13) # (stepsize, max_itt, tolerance)\n","print(torch.cuda.memory_summary(device=None, abbreviated=False))\n","noise_level = 0\n","\n","for j in range(len(z_dim_list)):\n","  z_dim = z_dim_list[j]\n","  for k in range(0, len(x_dim_list)):\n","    x_dim = x_dim_list[k]\n","    net_param = [z_dim, x_dim, x_dim, x_dim]    \n","    print('====> network parameter = {}'.format(net_param))\n","    for trials in range(tot_trials):  \n","      # print('====> trial number = {}'.format(trials))\n","\n","      generator = Generator_d(net_param)\n","      # normalized signal\n","      z0 = torch.randn(net_param[0]).cuda()\n","      z0 = z0 /torch.norm(z0)\n","\n","\n","      for i in range(0, depth):\n","        generator.f[i].cuda()\n","        generator.f[i].requires_grad = False\n","\n","      Wm = list(generator.f[0].parameters())[0].data\n","      for i in range(1, depth):\n","        Wm = torch.matmul(list(generator.f[i].parameters())[0].data,Wm)\n","\n","      A = torch.eye(x_dim).cuda()\n","\n","      # measurement for denoising\n","      y0 = generator(z0);\n","\n","      z_quasi_est, itt = quasi_grad(net_param, quasi_algo_param, generator, y0, Wm , A, 0)\n","      error_matrix_quasi[j,k, trials] = torch.norm(generator(z0).detach()-generator(z_quasi_est).detach())/ torch.norm(generator(z0).detach())\n","      \n","      del generator.f\n","      del generator\n","      del Wm\n","      del A\n","      del y0\n","\n","      del z0\n","      del z_quasi_est\n","\n","      torch.cuda.empty_cache()\n","      gc.collect()\n","  \n","    del net_param\n","    gc.collect()\n","  gc.collect()\n","  # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n","\n","# save result\n","# torch.save(error_matrix_quasi,\"/content/drive/My Drive/Colab Notebooks/Linear_approx/Results/Expansive_compare/Expansive_noiseless_phase.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkVQDeW92Wig"},"source":["torch.save(error_matrix_quasi,\"/content/drive/My Drive/Colab Notebooks/Linear_approx/Results/Expansive_compare/Expansive_noiseless_phase.pt\")\n","error_matrix_quasi = torch.load('/content/drive/My Drive/Colab Notebooks/Linear_approx/Results/Expansive_compare/Expansive_noiseless_phase.pt')\n","error_matrix_total = torch.zeros(len(x_dim_list), len(z_dim_list))\n","for j in range(len(z_dim_list)):\n","  for k in range(len(x_dim_list)):\n","    for trials in range(tot_trials):  \n","      if error_matrix_quasi[j, k, trials] < 1e-5:\n","        error_matrix_total[ len(x_dim_list) - 1 - k, j] = error_matrix_total[len(x_dim_list) - 1 -  k, j] + 1\n","fig, ax = plt.subplots(1)\n","plt.imshow(torch.kron(error_matrix_total/tot_trials, torch.ones(1,1)), cmap = 'gray')\n","ax.set_xticks(np.arange(0,26,2))\n","ax.set_xticklabels(['1','3','5','7','9','11','13','15','17','19'])\n","ax.set_yticks(np.arange(0,26,2))\n","ax.set_yticklabels(np.arange(441,0,-40))\n","plt.xlabel('code dimension, $n_0$')\n","plt.ylabel('layer widths with $n_1 = n_2 = n_3$')\n","plt.plot(range(0,19), 22 - 20*np.multiply(z_dim_list,1)/20)\n","fig.savefig(\"/content/drive/My Drive/Colab Notebooks/Linear_approx/Results/Phase_plot_non_expanse.png\", dpi = 300)\n"],"execution_count":null,"outputs":[]}]}